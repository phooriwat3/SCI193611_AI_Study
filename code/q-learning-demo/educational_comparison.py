#!/usr/bin/env python3
"""
à¸ªà¸²à¸˜à¸´à¸•à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š Tabular Q-Learning à¸à¸±à¸š Neural Network Function Approximation
à¸ªà¸³à¸«à¸£à¸±à¸šà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¸ªà¸­à¸™
"""

from simple_q_learning import SimpleGridWorld, SimpleQLearning
from pure_python_dqn import PurePythonDQN

def educational_comparison():
    """à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¹à¸šà¸š step-by-step à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸ªà¸­à¸™"""
    
    print("ğŸ“š Tabular Q-Learning vs Neural Network Function Approximation")
    print("=" * 70)
    print()
    
    # à¸ªà¸£à¹‰à¸²à¸‡ environment
    env = SimpleGridWorld(size=4, difficulty='normal')
    print("ğŸŒ Environment Setup:")
    env.print_environment_info()
    env.print_grid()
    
    print("\n" + "="*70)
    print("ğŸ“Š à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š 2 à¹à¸™à¸§à¸—à¸²à¸‡")
    print("="*70)
    
    # 1. Tabular Q-Learning
    print("\nğŸ—‚ï¸  1. TABULAR Q-LEARNING")
    print("-" * 30)
    print("ğŸ’¡ à¹à¸™à¸§à¸„à¸´à¸”: à¹€à¸à¹‡à¸š Q-value à¹à¸¢à¸à¹à¸•à¹ˆà¸¥à¸° (state, action) pair")
    print("ğŸ“ à¸‚à¹‰à¸­à¸¡à¸¹à¸¥: Q-table à¸‚à¸™à¸²à¸” 16 states Ã— 4 actions = 64 values")
    
    tabular_agent = SimpleQLearning(
        n_states=16,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.3
    )
    
    print("ğŸƒ à¸à¸¶à¸ 300 episodes...")
    tabular_agent.train(env, episodes=300)
    
    env.reset()
    tab_reward, tab_steps, _ = tabular_agent.test(env, show_path=False)
    
    print(f"âœ… à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ: Reward = {tab_reward:.2f}, Steps = {tab_steps}")
    
    # à¹à¸ªà¸”à¸‡ Q-table à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™
    print("\nğŸ“‹ Q-table à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ (5 states à¹à¸£à¸):")
    for state in range(5):
        q_vals = tabular_agent.q_table[state]
        pos = env.state_to_pos(state)
        print(f"   State {state}{pos}: {[f'{x:.2f}' for x in q_vals]}")
    
    # 2. Neural Network DQN
    print("\nğŸ§  2. NEURAL NETWORK FUNCTION APPROXIMATION (DQN)")
    print("-" * 50)
    print("ğŸ’¡ à¹à¸™à¸§à¸„à¸´à¸”: à¹ƒà¸Šà¹‰ neural network à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰ function Q(s,a)")
    print("ğŸ“ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡: Input(16) â†’ Hidden(32) â†’ Output(4)")
    print("ğŸ”— à¸à¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œ: ~580 weights à¹ƒà¸™ neural network")
    
    dqn_agent = PurePythonDQN(
        state_size=16,
        action_size=4,
        learning_rate=0.02,
        epsilon=1.0,
        epsilon_decay=0.99
    )
    
    print("ğŸƒ à¸à¸¶à¸ 300 episodes...")
    dqn_agent.train(env, episodes=300, verbose=False)
    
    env.reset()
    dqn_reward, dqn_steps, _ = dqn_agent.test(env, show_path=False)
    
    print(f"âœ… à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ: Reward = {dqn_reward:.2f}, Steps = {dqn_steps}")
    
    # à¹à¸ªà¸”à¸‡ neural network predictions
    print("\nğŸ“‹ Neural Network Q-values à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ (5 states à¹à¸£à¸):")
    for state in range(5):
        features = dqn_agent.state_to_features(state)
        q_vals = dqn_agent.q_network.predict(features)
        pos = env.state_to_pos(state)
        print(f"   State {state}{pos}: {[f'{x:.2f}' for x in q_vals]}")
    
    # 3. à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š
    print("\n" + "="*70)
    print("ğŸ” à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š")
    print("="*70)
    
    print(f"\nğŸ“Š à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ:")
    print(f"   Tabular Q-Learning: {tab_reward:.2f} reward, {tab_steps} steps")
    print(f"   Neural Network DQN: {dqn_reward:.2f} reward, {dqn_steps} steps")
    
    if abs(tab_reward - dqn_reward) < 1.0:
        winner = "à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹ƒà¸à¸¥à¹‰à¹€à¸„à¸µà¸¢à¸‡à¸à¸±à¸™"
    elif tab_reward > dqn_reward:
        winner = "Tabular Q-Learning à¸”à¸µà¸à¸§à¹ˆà¸²"
    else:
        winner = "Neural Network DQN à¸”à¸µà¸à¸§à¹ˆà¸²"
    
    print(f"   ğŸ† {winner}")
    
    print(f"\nğŸ“ˆ à¸‚à¹‰à¸­à¸”à¸µ/à¸‚à¹‰à¸­à¹€à¸ªà¸µà¸¢:")
    print("   Tabular Q-Learning:")
    print("   âœ… à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸£à¹‡à¸§, à¹€à¸ªà¸–à¸µà¸¢à¸£, à¹à¸™à¹ˆà¸™à¸­à¸™")
    print("   âŒ à¸ˆà¸³à¸à¸±à¸”à¸”à¹‰à¸§à¸¢à¸‚à¸™à¸²à¸” state space")
    print("   ğŸ¯ à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š: à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆ state space à¹€à¸¥à¹‡à¸")
    
    print("\n   Neural Network DQN:")
    print("   âœ… à¸£à¸­à¸‡à¸£à¸±à¸š state space à¹ƒà¸«à¸à¹ˆ, generalization")
    print("   âŒ à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸Šà¹‰à¸², à¹„à¸¡à¹ˆà¹€à¸ªà¸–à¸µà¸¢à¸£, à¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™")
    print("   ğŸ¯ à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š: à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆ state space à¹ƒà¸«à¸à¹ˆà¸¡à¸²à¸")
    
    print(f"\nğŸ“ à¸šà¸—à¹€à¸£à¸µà¸¢à¸™à¸ªà¸³à¸„à¸±à¸:")
    print("   â€¢ à¹ƒà¸™à¸›à¸±à¸à¸«à¸² Grid World à¹€à¸¥à¹‡à¸ â†’ Tabular à¸¡à¸±à¸à¸ˆà¸°à¸”à¸µà¸à¸§à¹ˆà¸²")
    print("   â€¢ à¹ƒà¸™à¸›à¸±à¸à¸«à¸²à¹ƒà¸«à¸à¹ˆ (à¹€à¸Šà¹ˆà¸™ Atari games) â†’ Neural Network à¸ˆà¸³à¹€à¸›à¹‡à¸™")
    print("   â€¢ Function approximation à¸Šà¹ˆà¸§à¸¢à¹ƒà¸«à¹‰ generalize à¹„à¸”à¹‰")
    print("   â€¢ Trade-off à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ simplicity à¸à¸±à¸š scalability")

def demonstrate_learning_curves():
    """à¹à¸ªà¸”à¸‡à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹à¸šà¸š step-by-step"""
    print("\n" + "="*70)
    print("ğŸ“ˆ à¸à¸²à¸£à¸ªà¸²à¸˜à¸´à¸•à¹€à¸ªà¹‰à¸™à¹‚à¸„à¹‰à¸‡à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰")
    print("="*70)
    
    env = SimpleGridWorld(size=3, difficulty='easy')  # à¹ƒà¸Šà¹‰ environment à¹€à¸¥à¹‡à¸à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¸Šà¸±à¸”à¹€à¸ˆà¸™
    print("à¹ƒà¸Šà¹‰ Grid World 3x3 à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¸‡à¹ˆà¸²à¸¢:")
    env.print_grid()
    
    # à¸ªà¸£à¹‰à¸²à¸‡ agents
    tabular = SimpleQLearning(9, 4, learning_rate=0.1, epsilon=0.3)
    dqn = PurePythonDQN(9, 4, learning_rate=0.05, epsilon=0.5)
    
    print("\nà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸—à¸µà¸¥à¸° 25 episodes:")
    
    for episode_batch in [25, 50, 75, 100]:
        print(f"\nEpisode {episode_batch}:")
        
        # à¸à¸¶à¸
        tabular.train(env, episodes=25)
        dqn.train(env, episodes=25, verbose=False)
        
        # à¸—à¸”à¸ªà¸­à¸š
        env.reset()
        tab_r, tab_s, _ = tabular.test(env, show_path=False)
        env.reset()
        dqn_r, dqn_s, _ = dqn.test(env, show_path=False)
        
        print(f"  Tabular: {tab_r:.1f} reward, {tab_s} steps")
        print(f"  DQN:     {dqn_r:.1f} reward, {dqn_s} steps")
    
    print("\nğŸ’¡ à¸ªà¸±à¸‡à¹€à¸à¸•: Tabular à¸¡à¸±à¸à¸ˆà¸° converge à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸²à¹ƒà¸™à¸›à¸±à¸à¸«à¸²à¹€à¸¥à¹‡à¸")

def main():
    """à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸¥à¸±à¸"""
    print("ğŸ¯ Educational Demo: Tabular vs Function Approximation")
    print("à¸ªà¸³à¸«à¸£à¸±à¸šà¹ƒà¸Šà¹‰à¸›à¸£à¸°à¸à¸­à¸šà¸à¸²à¸£à¸ªà¸­à¸™ Reinforcement Learning")
    print()
    
    try:
        educational_comparison()
        demonstrate_learning_curves()
        
        print(f"\nğŸ‰ à¸à¸²à¸£à¸ªà¸²à¸˜à¸´à¸•à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ!")
        print("=" * 70)
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
