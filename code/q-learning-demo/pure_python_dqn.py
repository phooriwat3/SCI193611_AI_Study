"""
Pure Python Deep Q-Network Implementation
‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ neural network ‡πÅ‡∏ó‡∏ô Q-table ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö function approximation
‡πÉ‡∏ä‡πâ Python ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á package ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
"""

import random
import math
from collections import deque

class SimpleMLP:
    """Multi-Layer Perceptron ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÉ‡∏ä‡πâ pure Python"""
    
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # ‡∏™‡∏∏‡πà‡∏° weights ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
        self.W1 = [[random.gauss(0, 0.1) for _ in range(hidden_size)] 
                   for _ in range(input_size)]
        self.b1 = [0.0] * hidden_size
        self.W2 = [[random.gauss(0, 0.1) for _ in range(output_size)] 
                   for _ in range(hidden_size)]
        self.b2 = [0.0] * output_size
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backpropagation
        self.hidden_output = []
        self.final_output = []
    
    def sigmoid(self, x):
        """Sigmoid activation function"""
        try:
            return 1 / (1 + math.exp(-max(-500, min(500, x))))
        except:
            return 0.5
    
    def forward(self, inputs):
        """Forward propagation"""
        # Hidden layer
        self.hidden_output = []
        for j in range(self.hidden_size):
            sum_val = self.b1[j]
            for i in range(self.input_size):
                sum_val += inputs[i] * self.W1[i][j]
            self.hidden_output.append(self.sigmoid(sum_val))
        
        # Output layer (linear)
        self.final_output = []
        for k in range(self.output_size):
            sum_val = self.b2[k]
            for j in range(self.hidden_size):
                sum_val += self.hidden_output[j] * self.W2[j][k]
            self.final_output.append(sum_val)
        
        return self.final_output[:]
    
    def train(self, inputs, targets):
        """Train with one sample"""
        # Forward pass
        outputs = self.forward(inputs)
        
        # Calculate output layer errors
        output_errors = []
        for k in range(self.output_size):
            error = targets[k] - outputs[k]
            output_errors.append(error)
        
        # Calculate hidden layer errors
        hidden_errors = []
        for j in range(self.hidden_size):
            error = 0.0
            for k in range(self.output_size):
                error += output_errors[k] * self.W2[j][k]
            # Sigmoid derivative
            sigmoid_derivative = self.hidden_output[j] * (1 - self.hidden_output[j])
            hidden_errors.append(error * sigmoid_derivative)
        
        # Update output layer weights
        for j in range(self.hidden_size):
            for k in range(self.output_size):
                self.W2[j][k] += self.learning_rate * output_errors[k] * self.hidden_output[j]
        for k in range(self.output_size):
            self.b2[k] += self.learning_rate * output_errors[k]
        
        # Update hidden layer weights
        for i in range(self.input_size):
            for j in range(self.hidden_size):
                self.W1[i][j] += self.learning_rate * hidden_errors[j] * inputs[i]
        for j in range(self.hidden_size):
            self.b1[j] += self.learning_rate * hidden_errors[j]
        
        return outputs
    
    def predict(self, inputs):
        """Predict without updating weights"""
        return self.forward(inputs)
    
    def copy_weights_from(self, other_network):
        """Copy weights from another network"""
        for i in range(self.input_size):
            for j in range(self.hidden_size):
                self.W1[i][j] = other_network.W1[i][j]
        
        for j in range(self.hidden_size):
            for k in range(self.output_size):
                self.W2[j][k] = other_network.W2[j][k]
            self.b1[j] = other_network.b1[j]
        
        for k in range(self.output_size):
            self.b2[k] = other_network.b2[k]

class PurePythonDQN:
    """Pure Python DQN Agent"""
    
    def __init__(self, state_size, action_size, learning_rate=0.01, 
                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,
                 memory_size=1000, batch_size=16, target_update=50):
        
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=memory_size)
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.target_update = target_update
        self.update_count = 0
        
        # Neural networks
        self.q_network = SimpleMLP(state_size, 32, action_size, learning_rate)
        self.target_network = SimpleMLP(state_size, 32, action_size, learning_rate)
        
        # Copy weights to target network
        self.update_target_network()
        
        # Statistics
        self.episode_rewards = []
    
    def update_target_network(self):
        """Copy weights from main network to target network"""
        self.target_network.copy_weights_from(self.q_network)
    
    def remember(self, state, action, reward, next_state, done):
        """Store experience in memory"""
        self.memory.append((state, action, reward, next_state, done))
    
    def get_action(self, state, training=True):
        """Choose action using epsilon-greedy policy"""
        if training and random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        
        # Convert state to feature vector
        features = self.state_to_features(state)
        q_values = self.q_network.predict(features)
        
        # Find action with max Q-value
        max_q = max(q_values)
        max_actions = [i for i, q in enumerate(q_values) if q == max_q]
        return random.choice(max_actions)
    
    def state_to_features(self, state):
        """Convert state to feature vector (one-hot encoding)"""
        features = [0.0] * self.state_size
        if 0 <= state < self.state_size:
            features[state] = 1.0
        return features
    
    def replay(self):
        """Train the network on a batch of experiences"""
        if len(self.memory) < self.batch_size:
            return
        
        # Sample batch
        batch = random.sample(self.memory, self.batch_size)
        
        for state, action, reward, next_state, done in batch:
            # Current Q-values
            current_features = self.state_to_features(state)
            current_q_values = self.q_network.predict(current_features)
            
            # Target Q-value
            if done:
                target_q = reward
            else:
                next_features = self.state_to_features(next_state)
                next_q_values = self.target_network.predict(next_features)
                target_q = reward + 0.95 * max(next_q_values)
            
            # Update target
            target_q_values = current_q_values[:]
            target_q_values[action] = target_q
            
            # Train
            self.q_network.train(current_features, target_q_values)
        
        # Update target network periodically
        self.update_count += 1
        if self.update_count % self.target_update == 0:
            self.update_target_network()
        
        # Decay epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def train_episode(self, env, max_steps=100):
        """Train one episode"""
        state = env.reset()
        total_reward = 0
        steps = 0
        
        for step in range(max_steps):
            action = self.get_action(state)
            next_state, reward, done = env.step(action)
            
            self.remember(state, action, reward, next_state, done)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        # Learn from experience
        self.replay()
        
        return total_reward, steps
    
    def train(self, env, episodes=500, verbose=True):
        """Train for multiple episodes"""
        for episode in range(episodes):
            reward, steps = self.train_episode(env)
            self.episode_rewards.append(reward)
            
            if verbose and episode % 100 == 0:
                avg_reward = sum(self.episode_rewards[-50:]) / min(50, len(self.episode_rewards))
                print(f"Episode {episode}: Avg Reward = {avg_reward:.2f}, "
                      f"Epsilon = {self.epsilon:.3f}, Memory = {len(self.memory)}")
    
    def test(self, env, show_path=False):
        """Test the trained agent"""
        state = env.reset()
        total_reward = 0
        steps = 0
        path = []
        action_names = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']
        
        if show_path:
            print("Pure Python DQN ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô:")
            env.print_grid()
        
        for step in range(50):
            action = self.get_action(state, training=False)
            next_state, reward, done = env.step(action)
            
            total_reward += reward
            steps += 1
            path.append(env.state_to_pos(state))
            
            if show_path:
                print(f"Step {step+1}: {env.state_to_pos(state)} -> {action_names[action]}")
                env.print_grid()
            
            state = next_state
            
            if done:
                if show_path:
                    print("‡∏ñ‡∏∂‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß!")
                break
        
        return total_reward, steps, path

# Import SimpleGridWorld ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
try:
    from simple_q_learning import SimpleGridWorld, SimpleQLearning
    HAS_SIMPLE_QL = True
except:
    HAS_SIMPLE_QL = False

def compare_tabular_vs_pure_dqn():
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Tabular Q-Learning ‡∏Å‡∏±‡∏ö Pure Python DQN"""
    if not HAS_SIMPLE_QL:
        print("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå simple_q_learning.py")
        return
    
    print("üß† ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Tabular Q-Learning vs Pure Python DQN")
    print("=" * 60)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á environment
    env = SimpleGridWorld(size=4, difficulty='normal')
    env.print_environment_info()
    env.print_grid()
    
    episodes = 400
    
    print("\n1. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Tabular Q-Learning:")
    print("-" * 30)
    
    # Tabular Q-Learning
    tabular_agent = SimpleQLearning(
        n_states=16,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.3
    )
    
    print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å Tabular Q-Learning...")
    tabular_agent.train(env, episodes=episodes)
    
    env.reset()
    tab_reward, tab_steps, _ = tabular_agent.test(env, show_path=False)
    print(f"Tabular - Reward: {tab_reward:.2f}, Steps: {tab_steps}")
    
    print("\n2. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Pure Python DQN:")
    print("-" * 30)
    
    # Pure Python DQN
    dqn_agent = PurePythonDQN(
        state_size=16,  # One-hot encoding
        action_size=4,
        learning_rate=0.02,
        epsilon=1.0,
        epsilon_decay=0.99
    )
    
    print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å Pure Python DQN...")
    dqn_agent.train(env, episodes=episodes)
    
    env.reset()
    dqn_reward, dqn_steps, _ = dqn_agent.test(env, show_path=False)
    print(f"Pure DQN - Reward: {dqn_reward:.2f}, Steps: {dqn_steps}")
    
    print("\n3. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:")
    print("-" * 30)
    print(f"Tabular Q-Learning: {tab_reward:.2f} reward, {tab_steps} steps")
    print(f"Pure Python DQN:    {dqn_reward:.2f} reward, {dqn_steps} steps")
    
    difference = abs(tab_reward - dqn_reward)
    if difference < 1.0:
        print("‚Üí ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô")
    elif tab_reward > dqn_reward:
        print("‚Üí Tabular Q-Learning ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ")
    else:
        print("‚Üí Pure Python DQN ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ")
    
    print("\nüìù ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:")
    print("- Tabular: ‡πÄ‡∏Å‡πá‡∏ö Q-value ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ state")
    print("- DQN: ‡πÉ‡∏ä‡πâ neural network ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ pattern")
    print("- ‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡πá‡∏Å: Tabular ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤")
    print("- ‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏´‡∏ç‡πà: DQN ‡∏à‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤")
    
    return tabular_agent, dqn_agent

def demonstrate_network_learning():
    """‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á neural network"""
    print("\nüîç ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á Neural Network")
    print("=" * 50)
    
    if not HAS_SIMPLE_QL:
        return
    
    env = SimpleGridWorld(size=3, difficulty='easy')
    print("‡πÉ‡∏ä‡πâ Grid World 3x3 ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:")
    env.print_grid()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DQN agent
    agent = PurePythonDQN(
        state_size=9,
        action_size=4,
        learning_rate=0.05,
        epsilon=0.5
    )
    
    print("\n‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡∏•‡∏∞ episode:")
    
    for episode in [0, 50, 100, 200]:
        if episode > 0:
            # ‡∏ù‡∏∂‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°
            agent.train(env, episodes=50, verbose=False)
        
        print(f"\nEpisode {episode}:")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Q-values ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ state
        print("Q-values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ state:")
        for state in range(9):
            features = agent.state_to_features(state)
            q_values = agent.q_network.predict(features)
            best_action = q_values.index(max(q_values))
            action_names = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']
            pos = env.state_to_pos(state)
            print(f"  State {state} {pos}: Best={action_names[best_action]} "
                  f"(Q={max(q_values):.2f})")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö performance
        env.reset()
        reward, steps, _ = agent.test(env, show_path=False)
        print(f"  Performance: {reward:.2f} reward, {steps} steps")

def interactive_dqn_demo():
    """Demo DQN ‡πÅ‡∏ö‡∏ö interactive"""
    if not HAS_SIMPLE_QL:
        print("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå simple_q_learning.py")
        return
    
    print("\nüéÆ Interactive Pure Python DQN Demo")
    print("=" * 40)
    
    # ‡∏£‡∏±‡∏ö input ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    try:
        size = int(input("‡∏Ç‡∏ô‡∏≤‡∏î grid (3-5, default=4): ") or "4")
        difficulty = input("‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å (easy/normal/hard/maze, default=normal): ") or "normal"
        episodes = int(input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô episodes (100-1000, default=400): ") or "400")
        learning_rate = float(input("Learning rate (0.01-0.1, default=0.02): ") or "0.02")
    except ValueError:
        size, difficulty, episodes, learning_rate = 4, 'normal', 400, 0.02
        print("‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default")
    
    print(f"\n‡∏™‡∏£‡πâ‡∏≤‡∏á Pure Python DQN:")
    print(f"Grid: {size}x{size}, Difficulty: {difficulty}")
    print(f"Episodes: {episodes}, Learning Rate: {learning_rate}")
    
    env = SimpleGridWorld(size=size, difficulty=difficulty)
    env.print_environment_info()
    env.print_grid()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DQN agent
    agent = PurePythonDQN(
        state_size=size * size,
        action_size=4,
        learning_rate=learning_rate,
        epsilon=1.0,
        epsilon_decay=0.995
    )
    
    print("\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å...")
    agent.train(env, episodes=episodes, verbose=True)
    
    print("\n‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö:")
    env.reset()
    reward, steps, _ = agent.test(env, show_path=False)
    optimal = env.get_optimal_path_length()
    efficiency = (optimal / steps * 100) if steps > 0 else 0
    
    print(f"Final Reward: {reward:.2f}")
    print(f"Steps: {steps}")
    print(f"Optimal Path: {optimal} steps")
    print(f"Efficiency: {efficiency:.1f}%")
    
    # ‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    show_path = input("\n‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô? (y/n): ").lower() == 'y'
    if show_path:
        print("\n‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ:")
        agent.test(env, show_path=True)

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    print("üß† Pure Python Deep Q-Network Demo")
    print("Neural Network Function Approximation (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á package)")
    print("=" * 65)
    
    if not HAS_SIMPLE_QL:
        print("‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå simple_q_learning.py ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")
        print("   ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á SimpleGridWorld ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
        return
    
    print("‚úÖ Pure Python DQN ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á numpy ‡∏´‡∏£‡∏∑‡∏≠ pytorch)")
    
    while True:
        print("\n‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î:")
        print("1. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Tabular vs Pure Python DQN")
        print("2. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á Neural Network")
        print("3. Interactive DQN Demo")
        print("4. ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°")
        
        choice = input("\n‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (1-4): ").strip()
        
        if choice == '1':
            compare_tabular_vs_pure_dqn()
        elif choice == '2':
            demonstrate_network_learning()
        elif choice == '3':
            interactive_dqn_demo()
        elif choice == '4':
            print("‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
            break
        else:
            print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1-4")
        
        print("\n" + "="*65)

if __name__ == "__main__":
    main()
