"""
Simple Q-Learning Example (Pure Python)
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Q-learning ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÉ‡∏ä‡πâ Python ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á numpy ‡∏´‡∏£‡∏∑‡∏≠ matplotlib

‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: python simple_q_learning.py
"""

import random
import json

class SimpleGridWorld:
    """Grid World Environment ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢"""
    
    def __init__(self, size=4, difficulty='normal'):
        self.size = size
        self.start = (0, 0)
        self.goal = (size-1, size-1)
        self.difficulty = difficulty
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å
        self.obstacles = self._generate_obstacles(size, difficulty)
        
        # Actions: 0=up, 1=down, 2=left, 3=right
        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        self.action_names = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']
        
        self.reset()
    
    def _generate_obstacles(self, size, difficulty):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å"""
        if size < 3:
            return []
        
        if difficulty == 'easy':
            if size == 3:
                return [(1, 1)]
            elif size == 4:
                return [(1, 1), (2, 2)]
            elif size == 5:
                return [(1, 1), (2, 2), (3, 1)]
            else:  # size >= 6
                return [(1, 1), (2, 2), (3, 1), (1, 3)]
        
        elif difficulty == 'normal':
            if size == 3:
                return [(1, 1)]
            elif size == 4:
                return [(1, 1), (2, 1)]
            elif size == 5:
                return [(1, 1), (2, 1), (1, 3), (3, 2)]
            else:  # size >= 6
                return [(1, 1), (2, 1), (1, 3), (3, 2), (4, 3), (2, 4)]
        
        elif difficulty == 'hard':
            if size == 3:
                return [(1, 1), (1, 0)]
            elif size == 4:
                return [(1, 1), (2, 1), (1, 2), (3, 1)]
            elif size == 5:
                return [(1, 1), (2, 1), (1, 2), (3, 1), (1, 3), (3, 3)]
            else:  # size >= 6
                return [(1, 1), (2, 1), (1, 2), (3, 1), (1, 3), (3, 3), 
                       (4, 2), (2, 4), (4, 4), (0, 3)]
        
        elif difficulty == 'maze':
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Ç‡∏≤‡∏ß‡∏á‡∏Å‡∏ï
            if size == 4:
                return [(1, 0), (1, 2), (2, 0), (2, 2)]
            elif size == 5:
                return [(1, 0), (1, 2), (1, 4), (2, 2), (3, 0), (3, 2), (3, 4)]
            elif size == 6:
                return [(1, 0), (1, 2), (1, 4), (2, 2), (2, 4), 
                       (3, 0), (3, 2), (4, 0), (4, 2), (4, 4)]
            else:  # size >= 7
                obstacles = []
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á
                for i in range(1, size-1, 2):
                    for j in range(0, size-1, 2):
                        if (i, j) != self.start and (i, j) != self.goal:
                            obstacles.append((i, j))
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô
                for i in range(0, size-1, 2):
                    for j in range(1, size-1, 2):
                        if (i, j) != self.start and (i, j) != self.goal:
                            obstacles.append((i, j))
                return obstacles
        
        else:  # custom ‡∏´‡∏£‡∏∑‡∏≠ default
            return [(1, 1), (2, 1)] if size >= 4 else []
    
    def get_difficulty_info(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å"""
        info = {
            'easy': '‡∏á‡πà‡∏≤‡∏¢ - ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏ô‡πâ‡∏≠‡∏¢ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô',
            'normal': '‡∏õ‡∏Å‡∏ï‡∏¥ - ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ',
            'hard': '‡∏¢‡∏≤‡∏Å - ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏°‡∏≤‡∏Å ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå',
            'maze': '‡πÄ‡∏Ç‡∏≤‡∏ß‡∏á‡∏Å‡∏ï - ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡∏≤‡∏ß‡∏á‡∏Å‡∏ï ‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î'
        }
        return info.get(self.difficulty, '‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á')
    
    def count_obstacles(self):
        """‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ"""
        return len(self.obstacles)
    
    def get_optimal_path_length(self):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Manhattan distance)"""
        return abs(self.goal[0] - self.start[0]) + abs(self.goal[1] - self.start[1])
    
    def print_environment_info(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°"""
        print(f"Grid World {self.size}x{self.size}")
        print(f"‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å: {self.difficulty} - {self.get_difficulty_info()}")
        print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ: {self.count_obstacles()}")
        print(f"‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {self.get_optimal_path_length()} steps")
        print(f"‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {self.start}, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {self.goal}")
        if self.obstacles:
            print(f"‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ: {self.obstacles}")
        print()
    
    def reset(self):
        """‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"""
        self.pos = self.start
        return self.get_state()
    
    def get_state(self):
        """‡πÅ‡∏õ‡∏•‡∏á position ‡πÄ‡∏õ‡πá‡∏ô state number"""
        return self.pos[0] * self.size + self.pos[1]
    
    def state_to_pos(self, state):
        """‡πÅ‡∏õ‡∏•‡∏á state number ‡πÄ‡∏õ‡πá‡∏ô position"""
        return (state // self.size, state % self.size)
    
    def is_valid(self, pos):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ position ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        r, c = pos
        return 0 <= r < self.size and 0 <= c < self.size and pos not in self.obstacles
    
    def step(self, action):
        """‡∏ó‡∏≥ action ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (next_state, reward, done)"""
        dr, dc = self.actions[action]
        new_pos = (self.pos[0] + dr, self.pos[1] + dc)
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏î‡πâ ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏¥‡∏°
        if not self.is_valid(new_pos):
            new_pos = self.pos
        
        self.pos = new_pos
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì reward
        if self.pos == self.goal:
            reward = 10
            done = True
        elif self.pos in self.obstacles:
            reward = -5
            done = False
        else:
            reward = -0.1
            done = False
        
        return self.get_state(), reward, done
    
    def print_grid(self, q_table=None, policy=None):
        """‡πÅ‡∏™‡∏î‡∏á grid ‡∏û‡∏£‡πâ‡∏≠‡∏° agent position"""
        print("Grid World:")
        for r in range(self.size):
            row = []
            for c in range(self.size):
                pos = (r, c)
                if pos == self.pos:
                    row.append('A')  # Agent
                elif pos == self.start:
                    row.append('S')  # Start
                elif pos == self.goal:
                    row.append('G')  # Goal
                elif pos in self.obstacles:
                    row.append('X')  # Obstacle
                else:
                    if policy:
                        state = r * self.size + c
                        best_action = max(range(4), key=lambda a: policy[state][a])
                        row.append(self.action_names[best_action])
                    else:
                        row.append('.')
            print(' '.join(row))
        print()

class SimpleQLearning:
    """Q-Learning Agent ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢"""
    
    def __init__(self, n_states, n_actions, learning_rate=0.1, 
                 discount=0.9, epsilon=0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.lr = learning_rate
        self.gamma = discount
        self.epsilon = epsilon
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Q-table ‡πÄ‡∏õ‡πá‡∏ô list of lists
        self.q_table = [[0.0 for _ in range(n_actions)] for _ in range(n_states)]
        
        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        self.episode_rewards = []
    
    def get_action(self, state, training=True):
        """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏î‡πâ‡∏ß‡∏¢ epsilon-greedy"""
        if training and random.random() < self.epsilon:
            return random.randint(0, self.n_actions - 1)
        else:
            return max(range(self.n_actions), key=lambda a: self.q_table[state][a])
    
    def update(self, state, action, reward, next_state):
        """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Q-value"""
        # ‡∏´‡∏≤ max Q-value ‡∏Ç‡∏≠‡∏á next state
        max_next_q = max(self.q_table[next_state])
        
        # Q-learning update
        target = reward + self.gamma * max_next_q
        error = target - self.q_table[state][action]
        self.q_table[state][action] += self.lr * error
    
    def train_episode(self, env, max_steps=100):
        """‡∏ù‡∏∂‡∏Å 1 episode"""
        state = env.reset()
        total_reward = 0
        steps = 0
        
        for _ in range(max_steps):
            action = self.get_action(state)
            next_state, reward, done = env.step(action)
            
            self.update(state, action, reward, next_state)
            
            total_reward += reward
            steps += 1
            state = next_state
            
            if done:
                break
        
        return total_reward, steps
    
    def train(self, env, episodes=1000):
        """‡∏ù‡∏∂‡∏Å‡∏´‡∏•‡∏≤‡∏¢ episodes"""
        print(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å {episodes} episodes...")
        
        for episode in range(episodes):
            reward, steps = self.train_episode(env)
            self.episode_rewards.append(reward)
            
            # ‡∏•‡∏î epsilon
            if episode > 0 and episode % 100 == 0:
                self.epsilon = max(0.01, self.epsilon * 0.95)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
            if episode % 200 == 0:
                avg_reward = sum(self.episode_rewards[-50:]) / min(50, len(self.episode_rewards))
                print(f"Episode {episode}: Average reward = {avg_reward:.2f}, Epsilon = {self.epsilon:.3f}")
    
    def test(self, env, show_path=True):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö agent"""
        state = env.reset()
        total_reward = 0
        steps = 0
        path = []
        action_names = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']
        
        if show_path:
            print("‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô:")
            env.print_grid()
        
        for step in range(50):
            action = self.get_action(state, training=False)
            next_state, reward, done = env.step(action)
            
            total_reward += reward
            steps += 1
            path.append((env.state_to_pos(state), action_names[action]))
            
            if show_path:
                print(f"Step {step+1}: {env.state_to_pos(state)} -> {action_names[action]}")
                env.print_grid()
            
            state = next_state
            
            if done:
                print("‡∏ñ‡∏∂‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß!")
                break
        
        return total_reward, steps, path
    
    def print_q_table(self):
        """‡πÅ‡∏™‡∏î‡∏á Q-table"""
        print("Q-Table:")
        print("State |   ‚Üë   |   ‚Üì   |   ‚Üê   |   ‚Üí   ")
        print("-" * 40)
        for state in range(self.n_states):
            q_vals = self.q_table[state]
            print(f"{state:5d} | {q_vals[0]:5.2f} | {q_vals[1]:5.2f} | {q_vals[2]:5.2f} | {q_vals[3]:5.2f}")
        print()
    
    def get_policy(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á policy ‡∏à‡∏≤‡∏Å Q-table"""
        policy = []
        for state in range(self.n_states):
            policy_state = [0.0] * self.n_actions
            best_action = max(range(self.n_actions), key=lambda a: self.q_table[state][a])
            policy_state[best_action] = 1.0
            policy.append(policy_state)
        return policy

def demo_simple():
    """‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢"""
    print("=" * 60)
    print("Q-Learning Demo - Enhanced Version")
    print("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Q-learning ‡πÉ‡∏ô Grid World ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å")
    print("=" * 60)
    print()
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å
    print("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å:")
    print("1. Easy (‡∏á‡πà‡∏≤‡∏¢)")
    print("2. Normal (‡∏õ‡∏Å‡∏ï‡∏¥)")
    print("3. Hard (‡∏¢‡∏≤‡∏Å)")
    print("4. Maze (‡πÄ‡∏Ç‡∏≤‡∏ß‡∏á‡∏Å‡∏ï)")
    
    difficulty_map = {'1': 'easy', '2': 'normal', '3': 'hard', '4': 'maze'}
    choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (1-4, default=2): ").strip() or '2'
    difficulty = difficulty_map.get(choice, 'normal')
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á environment
    env = SimpleGridWorld(size=5, difficulty=difficulty)
    env.print_environment_info()
    env.print_grid()
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á agent
    agent = SimpleQLearning(
        n_states=env.size * env.size,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.3  # ‡πÄ‡∏û‡∏¥‡πà‡∏° exploration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö environment ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
    )
    
    print("‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö random policy:")
    reward, steps, _ = agent.test(env, show_path=False)
    print(f"Reward: {reward:.2f}, Steps: {steps}")
    print()
    
    # ‡∏ù‡∏∂‡∏Å agent (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô episodes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö environment ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô)
    episodes = 1500 if difficulty in ['hard', 'maze'] else 1000
    print(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å {episodes} episodes...")
    agent.train(env, episodes=episodes)
    print()
    
    print("‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö learned policy:")
    reward, steps, path = agent.test(env, show_path=False)  # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á path ‡πÉ‡∏ô overview
    print(f"Final reward: {reward:.2f}, Steps: {steps}")
    optimal_steps = env.get_optimal_path_length()
    efficiency = (optimal_steps / steps * 100) if steps > 0 else 0
    print(f"‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û: {efficiency:.1f}% (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î {optimal_steps} steps)")
    print()
    
    # ‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π details ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    show_details = input("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π Q-table ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô? (y/n): ").lower() == 'y'
    if show_details:
        print("\nQ-Table (10 states ‡πÅ‡∏£‡∏Å):")
        print("State |   ‚Üë   |   ‚Üì   |   ‚Üê   |   ‚Üí   ")
        print("-" * 40)
        for state in range(min(10, env.size * env.size)):
            q_vals = agent.q_table[state]
            print(f"{state:5d} | {q_vals[0]:5.2f} | {q_vals[1]:5.2f} | {q_vals[2]:5.2f} | {q_vals[3]:5.2f}")
        
        print("\n‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô:")
        agent.test(env, show_path=True)
    
    # ‡πÅ‡∏™‡∏î‡∏á learned policy
    print("Learned Policy (‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ state):")
    policy = agent.get_policy()
    env.reset()
    env.print_grid(policy=policy)
    
    # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
    print("‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:")
    print(f"Episode ‡πÅ‡∏£‡∏Å 10 ‡∏ï‡∏≠‡∏ô - Average reward: {sum(agent.episode_rewards[:10])/10:.2f}")
    print(f"Episode ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ 10 ‡∏ï‡∏≠‡∏ô - Average reward: {sum(agent.episode_rewards[-10:])/10:.2f}")
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å
    print(f"\n‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å:")
    print(f"- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ: {env.count_obstacles()}")
    print(f"- ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏ï‡πà‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà: {env.count_obstacles()/(env.size*env.size)*100:.1f}%")
    success_rate = sum(1 for r in agent.episode_rewards[-100:] if r > 5) / min(100, len(agent.episode_rewards)) * 100
    print(f"- ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡πâ‡∏≤‡∏¢: {success_rate:.1f}%")

def interactive_demo():
    """‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡πÅ‡∏ö‡∏ö interactive"""
    print("=" * 60)
    print("Q-Learning Interactive Demo - Enhanced")
    print("=" * 60)
    print()
    
    # ‡∏£‡∏±‡∏ö input
    try:
        size = int(input("‡∏Ç‡∏ô‡∏≤‡∏î grid (3-8, default=5): ") or "5")
        
        print("\n‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å:")
        print("1. Easy (‡∏á‡πà‡∏≤‡∏¢)")
        print("2. Normal (‡∏õ‡∏Å‡∏ï‡∏¥)")  
        print("3. Hard (‡∏¢‡∏≤‡∏Å)")
        print("4. Maze (‡πÄ‡∏Ç‡∏≤‡∏ß‡∏á‡∏Å‡∏ï)")
        difficulty_map = {'1': 'easy', '2': 'normal', '3': 'hard', '4': 'maze'}
        diff_choice = input("‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å (1-4, default=2): ").strip() or '2'
        difficulty = difficulty_map.get(diff_choice, 'normal')
        
        lr = float(input("Learning rate (0.01-1.0, default=0.1): ") or "0.1")
        epsilon = float(input("Epsilon (0.01-1.0, default=0.3): ") or "0.3")
        episodes = int(input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô episodes (500-3000, default=1500): ") or "1500")
    except ValueError:
        size, difficulty, lr, epsilon, episodes = 5, 'normal', 0.1, 0.3, 1500
        print("‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default")
    
    print(f"\n‡∏™‡∏£‡πâ‡∏≤‡∏á Grid World {size}x{size} - {difficulty}")
    print(f"‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå: lr={lr}, epsilon={epsilon}, episodes={episodes}")
    print()
    
    # ‡∏£‡∏±‡∏ô experiment
    env = SimpleGridWorld(size=size, difficulty=difficulty)
    env.print_environment_info()
    
    agent = SimpleQLearning(
        n_states=size * size,
        n_actions=4,
        learning_rate=lr,
        discount=0.9,
        epsilon=epsilon
    )
    
    env.print_grid()
    agent.train(env, episodes=episodes)
    
    print("\n‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:")
    reward, steps, _ = agent.test(env, show_path=False)
    optimal_steps = env.get_optimal_path_length()
    efficiency = (optimal_steps / steps * 100) if steps > 0 else 0
    print(f"Final performance: Reward={reward:.2f}, Steps={steps}")
    print(f"‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û: {efficiency:.1f}% (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î {optimal_steps} steps)")
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å
    print(f"\n‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:")
    print(f"- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ: {env.count_obstacles()}")
    success_rate = sum(1 for r in agent.episode_rewards[-100:] if r > 5) / min(100, len(agent.episode_rewards)) * 100
    print(f"- ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {success_rate:.1f}%")
    
    # ‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π details ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    show_details = input("\\n‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π Q-table ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á? (y/n): ").lower() == 'y'
    if show_details:
        agent.print_q_table()
        print("\\n‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á:")
        agent.test(env, show_path=True)

def compare_difficulties():
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÜ"""
    print("=" * 60)
    print("Difficulty Comparison Demo")
    print("‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÜ")
    print("=" * 60)
    print()
    
    difficulties = ['easy', 'normal', 'hard', 'maze']
    size = 5
    episodes = 1000
    results = {}
    
    print(f"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡∏Å‡∏£‡∏¥‡∏î {size}x{size} ‡∏î‡πâ‡∏ß‡∏¢ {episodes} episodes ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö")
    print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà...\n")
    
    for diff in difficulties:
        print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö: {diff}")
        
        env = SimpleGridWorld(size=size, difficulty=diff)
        agent = SimpleQLearning(
            n_states=size * size,
            n_actions=4,
            learning_rate=0.1,
            discount=0.9,
            epsilon=0.3
        )
        
        agent.train(env, episodes=episodes)
        reward, steps, _ = agent.test(env, show_path=False)
        
        optimal_steps = env.get_optimal_path_length()
        success_rate = sum(1 for r in agent.episode_rewards[-100:] if r > 5) / min(100, len(agent.episode_rewards)) * 100
        
        results[diff] = {
            'obstacles': env.count_obstacles(),
            'final_reward': reward,
            'final_steps': steps,
            'optimal_steps': optimal_steps,
            'efficiency': (optimal_steps / steps * 100) if steps > 0 else 0,
            'success_rate': success_rate,
            'avg_early': sum(agent.episode_rewards[:100]) / 100,
            'avg_late': sum(agent.episode_rewards[-100:]) / 100
        }
        
        print(f"  ‚úì ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß - Final reward: {reward:.2f}, Steps: {steps}")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    print("\n" + "=" * 60)
    print("‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:")
    print("=" * 60)
    
    headers = ["‡∏£‡∏∞‡∏î‡∏±‡∏ö", "‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ", "Reward", "Steps", "‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"]
    print(f"{headers[0]:<8} {headers[1]:<8} {headers[2]:<8} {headers[3]:<6} {headers[4]:<12} {headers[5]:<10}")
    print("-" * 60)
    
    for diff in difficulties:
        r = results[diff]
        print(f"{diff:<8} {r['obstacles']:<8} {r['final_reward']:<8.2f} {r['final_steps']:<6} "
              f"{r['efficiency']:<12.1f}% {r['success_rate']:<10.1f}%")
    
    print("\nüìä ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:")
    print("- ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô")
    print("- Maze level ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô")
    print("- ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏à‡∏∞‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô")

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    print("ü§ñ Enhanced Q-Learning Demo")
    print("‡πÉ‡∏ä‡πâ Python ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á package ‡πÄ‡∏û‡∏¥‡πà‡∏°")
    print("üÜï ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö!")
    print()
    
    while True:
        print("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î:")
        print("1. Demo ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ)")
        print("2. Interactive Demo (‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå)")
        print("3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å (‡πÉ‡∏´‡∏°‡πà!)")
        print("4. ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°")
        print()
        
        choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (1-4): ").strip()
        
        if choice == '1':
            demo_simple()
        elif choice == '2':
            interactive_demo()
        elif choice == '3':
            compare_difficulties()
        elif choice == '4':
            print("‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
            break
        else:
            print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1-4")
        
        print("\n" + "="*60 + "\n")

if __name__ == "__main__":
    main()
    main()
