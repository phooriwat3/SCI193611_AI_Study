#!/usr/bin/env python3
"""
à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¸¢à¸²à¸à¹ƒà¸«à¸¡à¹ˆà¸‚à¸­à¸‡ Q-learning
"""

from simple_q_learning import SimpleGridWorld, SimpleQLearning

def test_all_difficulties():
    """à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¸¢à¸²à¸à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”"""
    print("ðŸ”¥ à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¸¢à¸²à¸à¹ƒà¸«à¸¡à¹ˆ!")
    print("=" * 50)
    
    difficulties = ['easy', 'normal', 'hard', 'maze']
    size = 5
    
    for diff in difficulties:
        print(f"\nðŸ“Š à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸”à¸±à¸š: {diff.upper()}")
        print("-" * 30)
        
        # à¸ªà¸£à¹‰à¸²à¸‡ environment
        env = SimpleGridWorld(size=size, difficulty=diff)
        env.print_environment_info()
        env.print_grid()
        
        # à¸ªà¸£à¹‰à¸²à¸‡ agent
        agent = SimpleQLearning(
            n_states=size * size,
            n_actions=4,
            learning_rate=0.1,
            discount=0.9,
            epsilon=0.3
        )
        
        # à¸à¸¶à¸à¸ªà¸±à¹‰à¸™à¹† à¹€à¸žà¸·à¹ˆà¸­à¸”à¸¹à¸œà¸¥
        print("à¸à¸³à¸¥à¸±à¸‡à¸à¸¶à¸à¸ªà¸±à¹‰à¸™à¹†...")
        agent.train(env, episodes=300)
        
        # à¸—à¸”à¸ªà¸­à¸š
        reward, steps, _ = agent.test(env, show_path=False)
        optimal = env.get_optimal_path_length()
        
        print(f"à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ: Reward={reward:.2f}, Steps={steps}")
        print(f"à¹€à¸ªà¹‰à¸™à¸—à¸²à¸‡à¸—à¸µà¹ˆà¸ªà¸±à¹‰à¸™à¸—à¸µà¹ˆà¸ªà¸¸à¸”: {optimal} steps")
        print(f"à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž: {(optimal/steps*100) if steps > 0 else 0:.1f}%")

def test_maze_detailed():
    """à¸—à¸”à¸ªà¸­à¸šà¹‚à¸«à¸¡à¸” maze à¹à¸šà¸šà¸¥à¸°à¹€à¸­à¸µà¸¢à¸”"""
    print("\nðŸŒ€ à¸—à¸”à¸ªà¸­à¸šà¹‚à¸«à¸¡à¸” MAZE à¹à¸šà¸šà¸¥à¸°à¹€à¸­à¸µà¸¢à¸”")
    print("=" * 40)
    
    env = SimpleGridWorld(size=6, difficulty='maze')
    env.print_environment_info()
    env.print_grid()
    
    agent = SimpleQLearning(
        n_states=36,
        n_actions=4,
        learning_rate=0.1,
        discount=0.95,
        epsilon=0.4
    )
    
    print("à¸à¸¶à¸ 800 episodes...")
    agent.train(env, episodes=800)
    
    print("\nà¸—à¸”à¸ªà¸­à¸š learned policy:")
    reward, steps, path = agent.test(env, show_path=True)
    
    print(f"\nà¸ªà¸£à¸¸à¸›:")
    print(f"- Final Reward: {reward:.2f}")
    print(f"- Steps: {steps}")
    print(f"- Optimal Path: {env.get_optimal_path_length()} steps")
    print(f"- Efficiency: {(env.get_optimal_path_length()/steps*100) if steps > 0 else 0:.1f}%")

if __name__ == "__main__":
    test_all_difficulties()
    test_maze_detailed()
